---
title: 'sample - parallelized'
layout: "../../layouts/MDLayout.astro"
---

## Table of Contents

## Overview
This month, a B2B company is migrating its CSV files onto a database. These CSV files contain years of oddly formatted data
that was not standardized. The company understands that doing such a task themselves might impose significant downtime and may not be worth the time 
taken to do it themselves. They have hired you, along with a team of other developers, to create a stop-gap solution for the time being by using a
hybrid approach.

They plan to expose an API that uses the CSV's themselves as a database in a monolithic approach so that server uptime is not affected while the large amount of data transfer is done.
Using CSV files is conventionally extremely inefficient as you would be using the disk to read/write which would be much slower (if you are interested, look into Amdahl's Law - basically, I/O bound tasks are sequential so cannot be sped up without changes to the hardware).
Thus, to combat this you plan to use a dictionary to act as an associative cache for the CSV data that would be important to keep on hand (such as data for ongoing projects). This dictionary would be stored in memory,
thus we can improve our I/O times for a lot of the operations.

<div>_*Please note this is a shorter sample prompt that just aims to show you what a prompt would look like*_</div><br></br>
## Task 1: Understanding the Problem
Let's take a look at a table visualization of what the data in one of the CSV's looks like:
| id | date | manager | tasks_csv_path | expected_cost | expected_payment |  ongoing | ... | 
| --- | ----- | ------------ | -------- | ------------- | ---------------- | ----- | ----- |
| 43 | 2008-12-13 | Lempel | https://website.com/assets/csvs/2008-12-13/tasks-43.csv | $32039.93 | $40940.00 | false| |
| 78 | 2009-02-17 | Welch | https://website.com/assets/csvs/2008-02-17/tasks-78.csv | $38731.04 | $41500.00 | false |  |
| 126 | 2023-09-28 | Ziv | https://website.com/assets/csvs/2023-09-28/tasks-126.csv | $63040.49 | $79000.00 | true | |

Our job is to essentially abstract away these details into *three functions*:
<div class="child:py-0 child:my-0">
1. parse_csv
 - Should parse the CSV file into a dictionary object (can use the 'csv' Python library)
2. get_project
    - Check if the project is already in the cache, if it is then return it
    - If not in the cache, get details from the CSV
    - Add or remove from the cache with `cache_fields`
3. cache_fields
- Here, a choice needs to be made on what method to implement for adding and removing elements from the "cache". A common policy for this is least-recently-used (or LRU). But, in this case, we expect that these projects won't be accessed again and the spatial and temporal locality from LRU may not be very useful. Maybe it's smarter to use a mostly static cache and for ongoing projects.
</div>

<br></br>

## Optimization
The optimization part of this sample problem is quite basic. It's essentially just a process of trying out different cache policies and experimenting with ways to read the CSV faster to get the maximum possible speedup over a naive approach.

<h2 id="readings" class="!mb-2">Readings</h2>
<h3 id="general" class="!mb-0 !ml-4">General</h3>
<div class="child:!mb-0 child:!mt-1 child:!ml-6">
- [Testing Framework](/sample-guidelines/testing-framework "An overview of the testing framework for the competition")
{/* - [Web Access](/guidelines/web-access "A list of allowed websites for the competition") */}
- [Resources](/sample-guidelines/resources "A list of resources for the competition with details about possible libraries for use")
</div>
{/*### Algorithm Implementation {.mt-0 .ml-4 .mb-0}
- [LZW Compression](lzw-compression "An overview of LZW compression")
- [LZW Decompression](lzw-decompression "An overview of LZW decompression")
{: .mb-0 .ml-6}*/}